{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Exam/Project -- Applied Machine Learning (CNN Portion)\n",
    "\n",
    "Josh Gregory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "For the specific convolutional neural network (CNN) architecture, I'm going to be using a CNN with the following shape:\n",
    "\n",
    "* Input layer of 32x32x3\n",
    "* Random horizontal flips, vertical flips, rotations, and zooms for data augmentation (improve model generalizability)\n",
    "\n",
    "This model can be found in the `final_cnn_small.py` file, as I wanted to have the Python file separate from this notebook. It's commented though, so feel free to look through it!\n",
    "\n",
    "### First Convolutional Block\n",
    "\n",
    "* Convolutional layer with 32 filters with a kernel size of 5x5\n",
    "* Batch normalization layer\n",
    "* Max pooling layer\n",
    "* ReLU activation function\n",
    "\n",
    "### Second Convolutional Block\n",
    "* Convolutional layer with 64 filters with a kernel size of 3x3\n",
    "* Batch normalization layer\n",
    "* Max pooling layer\n",
    "* ReLU activation function\n",
    "\n",
    "### Third Convolutional Block\n",
    "* Convolutional layer with 128 filters with a kernel size of 3x3\n",
    "* Batch normalization layer\n",
    "* Max pooling layer\n",
    "* ReLU activation function\n",
    "\n",
    "\n",
    "### Fully-Connected Layer\n",
    "* Flatten layer to vectorize all of the outputs from the previous ReLU.\n",
    "* Dense layer with 256 connections and a ReLU activation function\n",
    "* A dropout layer with a dropout probability of 50%\n",
    "* A second Dense layer that connects everything from the dropout layer to three output nodes, softmax activation function.\n",
    "\n",
    "In total, there are:\n",
    "\n",
    "Total params: 2,193,731 (8.37 MB)\n",
    "Trainable params: 2,193,347 (8.37 MB)\n",
    "Non-trainable params: 384 (1.50 KB)\n",
    "\n",
    "The `model.summary()` call returns the following:\n",
    "\n",
    "![image](./cnn_summary.png)\n",
    "\n",
    "with the overall CNN layout looking like this:\n",
    "\n",
    "<img src='./cnn_model.png' width='600' height='2000'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additions I Made\n",
    "\n",
    "There are a few additions I made to this CNN from what we did in class. I'll discuss them here.\n",
    "\n",
    "### Data Augmentation\n",
    "\n",
    "In the very beginning of my network (before any ML magic has even happened), I have a small data augmentation pipeline. I want my CNN to generalize well, which means that if I were to give it many images of traffic lights in the correct orientation, the model would most likely recognize the positions of the lights. That might be fine for some applications, but I wanted to force my CNN to learn more generalizable parameters. To accomplish this, I have random flips, rotations (both horizontal and vertical), and zooms that happen probabilistically to each image before it is passed to the CNN itself. This causes each image to not be \"perfect\" on purpose, forcing the model to learn different relationships in traffic lights besides just memorizing the order of the lights, because I remove that option by messing with the data.\n",
    "\n",
    "### Batch Normalization Layer\n",
    "\n",
    "Between the convolutional and max. pooling layers, I added a batch normalization layer. I did a bit more reading on CNNs before I started writing my CNN, and **many** people recommended including batch normalization layers. [This](https://www.geeksforgeeks.org/what-is-batch-normalization-in-cnn/) is one of the articles that I read about it and why it is used. [This lecture](https://cs231n.stanford.edu/slides/2022/lecture_6_jiajun.pdf) was also really great. Essentially, batch normalization allows for each batch of data to be normalized, sort of similar to what the `StandardScaler()` does in Scikit-Learn, however here instead of standardizing pixel intensities, we're standardizing the activations of each layer across a mini-batch during training. Batch normalization also serves to add some noise, helping with overfitting.\n",
    "\n",
    "### Dropout Layer\n",
    "\n",
    "We discussed dropout layers briefly in class, and I decided to add one at the very end of my network, right before the final Dense layer. This was added to prevent overfitting and to force the model to learn even more generalizable features.\n",
    "\n",
    "\n",
    "### Early Stopping Callback\n",
    "\n",
    "I added in an early stopping callback to prevent the model from overfitting, as an incredibly small loss, while good, could be a symptom of a model that has overfit. Here, I have the model stop training after the validation loss hasn't changed in 10 epochs. I experimented with larger numbers, however I found that the model would tend to bounce between extremely small validation losses that were technically different, so the model kept training. A number between 10-20 seemed to work the best.\n",
    "\n",
    "\n",
    "## Model Results\n",
    "\n",
    "The confusion matrix for the small model without hyperparameter optimization is:\n",
    "\n",
    "![image](./cnn_conf_mat_small.png)\n",
    "\n",
    "Instead of using the manual learning curve plotting like we did in class, I decided to use the [Weights & Biases](https://wandb.ai/site/experiment-tracking/) experiment tracking. It is what OpenAI used for their initial experimentations when designing ChatGPT, and it's what I'm going to use for my thesis, so I thought this project would be a good excuse to get more familiar with it. They have free academic and personal hosting tiers, which is also pretty great.\n",
    "\n",
    "The validation curves look as follows (training dataset is in solid blue, validation dataset is in dotted red):\n",
    "\n",
    "![image](./images/cnn_small_val_curve_acc.png)\n",
    "\n",
    "![image](./images/cnn_small_val_curve_loss.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "applied_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
