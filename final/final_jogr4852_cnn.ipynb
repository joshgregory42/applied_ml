{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Exam/Project -- Applied Machine Learning (CNN Portion)\n",
    "\n",
    "Josh Gregory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "For the specific convolutional neural network (CNN) architecture, I'm going to **DESCRIPTION GOES HERE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/josh/anaconda3/envs/applied_ml/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "\n",
    "# Doing experiment tracking in Weights and Biases, which is the same that I'm using for my thesis. Trying to use this to get some experience using it\n",
    "import wandb\n",
    "from wandb.integration.keras import WandbMetricsLogger\n",
    "\n",
    "# Also going to hyperparameter optimize using Optuna, again this is the same library that I'm using as in my thesis, so I'm going to use it here to get familiar with it.\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjag42\u001b[0m (\u001b[33mjag42-university-of-colorado-boulder\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm running this notebook on my laptop, which has a discrete NVIDIA GPU. Let's make sure TensorFlow can see it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-process images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_images(data_directory):\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    for color in ['red', 'yellow', 'green']:\n",
    "        color_path = os.path.join(data_directory, color)\n",
    "\n",
    "        # Get all files in color directory\n",
    "\n",
    "        for file in os.listdir(color_path):\n",
    "            img_path = os.path.join(color_path, file)\n",
    "                \n",
    "            # Read in image\n",
    "            img = cv2.imread(img_path)\n",
    "\n",
    "            image_crop = np.copy(img)\n",
    "            row_crop = 7\n",
    "            col_crop = 8\n",
    "            image_crop = img[row_crop:-row_crop, col_crop:-col_crop, :]\n",
    "\n",
    "            img_resized = cv2.resize(image_crop, (32, 32))\n",
    "\n",
    "            # Resize image\n",
    "            # img_resized = transform.resize(img, (32, 32))\n",
    "\n",
    "            # Flatten image\n",
    "            flat_features = img_resized.flatten()\n",
    "            features.append(flat_features)\n",
    "\n",
    "            # Append the label as well\n",
    "            labels.append(color)\n",
    "\n",
    "    features = np.array(features)\n",
    "\n",
    "    # Convert strings of colors to integer values\n",
    "    light_dict = {'red': 0, 'yellow': 1, 'green': 2}\n",
    "    labels = np.array([light_dict[label] for label in labels])\n",
    "    features, labels = shuffle(features, labels, random_state=42)\n",
    "\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_cnn(input_shape=(32, 32, 3), num_classes=3):\n",
    "    model = models.Sequential([\n",
    "\n",
    "        # First convolutional layer\n",
    "        layers.Conv2D(filters=32, kernel_size=(5, 5), strides=(1, 1), padding='same', data_format='channels_last', name='conv_1', activation='relu'\n",
    "        ),\n",
    "        # Second convolutional layer\n",
    "        layers.Conv2D(64, (3, 3), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Third convolutional layer\n",
    "        layers.Conv2D(128, (3, 3), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Flatten and Fully Connected Layers\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy']) # same as `tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')`\n",
    "\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    # Data augmentation to prevent overfitting\n",
    "    data_augmentation = models.Sequential([\n",
    "        layers.RandomFlip('horizontal'),\n",
    "        layers.RandomFlip('vertical'),\n",
    "        layers.RandomRotation(0.1),\n",
    "        layers.RandomZoom(0.1),\n",
    "    ])\n",
    "\n",
    "    # Create base model without data autmentation\n",
    "    model = create_base_cnn()\n",
    "\n",
    "    model = models.Sequential([\n",
    "        data_augmentation,\n",
    "        model\n",
    "    ])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    early_stopping = callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        restore_best_weights=True,\n",
    "        patience=5\n",
    "    )\n",
    "\n",
    "    reduce_lr = callbacks.ReduceLROnPlatea(\n",
    "        monitor='val_loss', \n",
    "        factor=0.2,\n",
    "        patience=5, \n",
    "        min_lr=0.001\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping, redice_lr, WandbMetricsLogger()]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # Example of generating random input data for testing\n",
    "    import numpy as np\n",
    "    x_test = np.random.rand(16, 64, 64, 3)  # 16 random images\n",
    "    y_test = np.random.rand(16, 3)  # Random one-hot encoded labels\n",
    "    y_test = y_test / y_test.sum(axis=1)[:, np.newaxis]  # Normalize to create valid probability distribution\n",
    "\n",
    "    train_model()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "applied_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
